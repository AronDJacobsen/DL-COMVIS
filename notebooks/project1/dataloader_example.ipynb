{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data.project1.dataloader import get_loaders\n",
    "from src.utils import invertNormalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from src.utils import set_seed\n",
    "\n",
    "class HotdogDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "def get_normalization_constants(root: str, seed: int = 0):\n",
    "    # Set seed for split control\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Define transforms (only resize as we want to compute means...)\n",
    "    normalization_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainvalset     = ImageFolder(f'{root}/train', transform=normalization_transform)\n",
    "\n",
    "    # Get size of validation split\n",
    "    N_trainval  = trainvalset.__len__()\n",
    "    N_val       = int(0.2 * N_trainval)\n",
    "    \n",
    "    # Get trainset\n",
    "    trainset    = torch.utils.data.Subset(trainvalset, range(N_val, N_trainval))     \n",
    "\n",
    "    # Compute means and standard deviations from training set\n",
    "    train_mean = torch.stack([t.mean(1).mean(1) for t, c in tqdm(trainset, desc='Computing mean of training split...')]).mean(0)\n",
    "    train_std  = torch.stack([t.std(1).std(1) for t, c in tqdm(trainset, desc='Computing std. dev. of training split...')]).std(0)\n",
    "    print(f\"\\nMean: {train_mean}\\nStd. dev.: {train_std}\")    \n",
    "    return train_mean, train_std\n",
    "\n",
    "def get_loaders(root: str = '/dtu/datasets1/02514/hotdog_nohotdog', batch_size: int = 64, seed: int = 0, train_transforms=None, test_transforms=None) -> dict:\n",
    "\n",
    "    # Set seed for split control\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Load images as datasets\n",
    "    trainvalset = ImageFolder(f'{root}/train') #, transform=train_transforms)\n",
    "    testset     = ImageFolder(f'{root}/test', transform=test_transforms)\n",
    "\n",
    "    # Get validation set size\n",
    "    N_trainval  = trainvalset.__len__()                                       # total training points\n",
    "    N_val       = int(0.2 * N_trainval)                                       # take ~20% for validation\n",
    "\n",
    "    # Split trainval dataset into train- and valset\n",
    "    val_subset      = torch.utils.data.Subset(trainvalset, range(N_val))         \n",
    "    train_subset    = torch.utils.data.Subset(trainvalset, range(N_val, N_trainval))     \n",
    "\n",
    "    valset = HotdogDataset(val_subset, transform=test_transforms)\n",
    "    trainset = HotdogDataset(train_subset, transform=train_transforms)\n",
    "\n",
    "    # Get dataloaders\n",
    "    trainloader = DataLoader(trainset,  batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    valloader   = DataLoader(valset,    batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    testloader  = DataLoader(testset,   batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    # Return loaders in dictionary\n",
    "    return {'train': trainloader, 'validation': valloader, 'test': testloader}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean of training split...: 100%|█| 1638/1638 [00:05<00:00, 319.54\n",
      "Computing std. dev. of training split...: 100%|█| 1638/1638 [00:06<00:00, 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean: tensor([0.5132, 0.4369, 0.3576])\n",
      "Std. dev.: tensor([0.0214, 0.0208, 0.0223])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT = '/dtu/datasets1/02514/hotdog_nothotdog'\n",
    "SEED = 0\n",
    "\n",
    "# Get normalization constants\n",
    "train_mean, train_std = get_normalization_constants(root=ROOT, seed=SEED)\n",
    "\n",
    "# Define transforms for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),         # flips \"left-right\"\n",
    "    # transforms.RandomVerticalFlip(p=1.0),           # flips \"upside-down\"\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.RandomRotation(degrees=(60, 70)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=train_mean, \n",
    "        std=train_std, \n",
    "    )\n",
    "])\n",
    "\n",
    "# Define transforms for test and validation\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=train_mean, \n",
    "        std=train_std, \n",
    "    )\n",
    "])\n",
    "\n",
    "# Get data loaders with applied transformations\n",
    "loaders = get_loaders(root=ROOT, batch_size=64, seed=SEED, train_transforms=train_transforms, test_transforms=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loaders\n",
    "trainloader = loaders['train']\n",
    "valloader   = loaders['validation']\n",
    "testloader  = loaders['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_examples(batch, targets, title, save_path, train_mean = [0.5132, 0.4369, 0.3576], train_std = [0.0214, 0.0208, 0.0223]):\n",
    "\n",
    "    # \"Re-normalize\" data\n",
    "    inverseNormalization = invertNormalization(train_mean=train_mean, train_std=train_std)\n",
    "    data                 = inverseNormalization(batch).clamp(0, 1)\n",
    "\n",
    "    # Get idx2class mapping\n",
    "    class2idx = trainloader.dataset.subset.dataset.class_to_idx\n",
    "    idx2class = {v: k for k, v in class2idx.items()}\n",
    "\n",
    "    # Plot examples from batch\n",
    "    fig, axs = plt.subplots(3, 10, figsize=(18, 5))\n",
    "    for i in tqdm(range(10), desc=title):\n",
    "        axs[0, i].imshow(data[i].permute(1,2,0).numpy())\n",
    "        axs[1, i].imshow(data[i+10].permute(1,2,0).numpy())\n",
    "        axs[2, i].imshow(data[i+20].permute(1,2,0).numpy())\n",
    "\n",
    "        axs[0, i].set_title(idx2class[targets[i].item()])\n",
    "        axs[1, i].set_title(idx2class[targets[i+10].item()])\n",
    "        axs[2, i].set_title(idx2class[targets[i+20].item()])\n",
    "\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].axis('off')\n",
    "        axs[2, i].axis('off')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}/{title}.png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set: 100%|███████████████████████| 10/10 [00:00<00:00, 200.39it/s]\n",
      "Validation set: 100%|█████████████████████| 10/10 [00:00<00:00, 209.55it/s]\n",
      "Test set: 100%|███████████████████████████| 10/10 [00:00<00:00, 198.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get example training batch\n",
    "batch, targets = next(iter(trainloader)) \n",
    "visualize_examples(batch, targets, \"Training set\", save_path=\"/zhome/b8/5/147299/Desktop\")\n",
    "\n",
    "# Get example validation batch\n",
    "batch, targets = next(iter(valloader)) \n",
    "visualize_examples(batch, targets, \"Validation set\", save_path=\"/zhome/b8/5/147299/Desktop\")\n",
    "\n",
    "# Get example test batch\n",
    "batch, targets = next(iter(testloader)) \n",
    "visualize_examples(batch, targets, \"Test set\", save_path=\"/zhome/b8/5/147299/Desktop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
